{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAIG Introduction to Deep Learning Workshop - Exercises\n",
    "\n",
    "Today, we'll be exploring deep neural networks and applying them to the MNIST dataset.\n",
    "\n",
    "By Swetha Revanur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "Run any code below by highlighting the box and hitting `Shift + Enter`. Import the libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1337)\n",
    "import tensorflow as tf\n",
    "tf.random.set_random_seed(2)\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from MNIST (70000 28x28 images total)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "num_train = x_train.shape[0]\n",
    "num_test = x_test.shape[0]\n",
    "\n",
    "print(num_train, 'records in train set')\n",
    "print(num_test, 'records in test set')\n",
    "print(num_train + num_test, 'records in total\\n')\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = x_train.shape[1] # dimension of square images = 28\n",
    "\n",
    "# flatten so we instead have 70000 28^2=784-dimensional vectors (one per image)\n",
    "x_train = x_train.reshape((x_train.shape[0], dim*dim))\n",
    "x_test = x_test.reshape((x_test.shape[0], dim*dim))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train)) # 10 unique digits (0-9)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 1: Create a Baseline Model\n",
    "\n",
    "**Your task:** At this point, we're ready to create a basic neural network classifier using Keras. Your model should have 6 layers, of sizes 10, 100, 500, 50, and `num_classes`. All layers, save the last one, should use a ReLU activation function. The last layer should use softmax. Check this [guide](https://keras.io/getting-started/sequential-model-guide/) out to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 15\n",
    "learning_rate = 5e-3\n",
    "\n",
    "#########################################################\n",
    "# Initializes baseline neural network.\n",
    "# ReLU and Softmax activations. Cross-entropy loss.\n",
    "#########################################################\n",
    "def baseline_classifier(learning_rate):\n",
    "    # create model\n",
    "    model_baseline = Sequential()\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    \n",
    "    # END CODE\n",
    "\n",
    "    # compile model\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model_baseline.compile(loss = keras.losses.categorical_crossentropy, optimizer = sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 2: Train and Evaluate the Baseline Model\n",
    "\n",
    "**Your task:** Use the `eval()` function below to train and evaluate our baseline model. The return value of `eval()` is a tuple of loss and accuracy. Print both of these. In `eval()`, feel free to change the value of the `verbose` parameter. When `verbose = 0`, no information is printed. When it's 5, a lot of detailed information about the training process gets printed. Your test accuracy for this basic model should be around 37%.\n",
    "\n",
    "Note that a parameter is `model.fit()` is `validation_split`. This takes a float from 0 to 1, representing the percentage of the training set to use for validation. Why do we need a validation set? More than training performance, we are interested in how our model does on unseen data. We can split data into only training and test. But if we then optimize our model using results from the test set, our test set can no longer be considered unseen data. As a workaround, we split our dataset into train, validation, and test. This way, we can optimize on our validation set, and only touch our test set at the very end. For the purposes of this workshop, we have ignored the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Trains and evaluates given model. Returns loss and \n",
    "# accuracy.\n",
    "#########################################################\n",
    "def eval(model, verb = 2):\n",
    "    # fit the model\n",
    "    model.fit(x_train, y_train, \n",
    "              epochs = epochs, \n",
    "              batch_size = batch_size, \n",
    "              verbose = verb,\n",
    "              shuffle = False)\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "model_baseline = baseline_classifier(learning_rate)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# call eval and print the test loss and accuracy\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 3: Introduce Regularization\n",
    "\n",
    "Sometimes our train accuracies are consistently higher than our test accuracy.  We might also see train losses that continue to decrease while the validation losses hit a minimum then increase. Both of these things are indicators of overfitting and poor model generalization. Regularization is a way to fix this. Regularization reduces overfitting by adding a penalty to the loss function. By adding this penalty, the model is trained such that it does not learn interdependent sets of features weights.\n",
    "\n",
    "**Your task:** Add L2 regularization to the last two hidden layers of our model. You may want to refer to the [Keras documentation about regularizers](https://keras.io/regularizers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_strength = 0.1\n",
    "\n",
    "#########################################################\n",
    "# Initializes neural network with L2 regularization.\n",
    "#########################################################\n",
    "def regularized_classifier(learning_rate, reg_strength):\n",
    "    # create model\n",
    "    model_regularized = Sequential()\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    \n",
    "    # END CODE\n",
    "    \n",
    "    # compile model\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model_regularized.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer = sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model_regularized\n",
    "\n",
    "model_regularized = regularized_classifier(learning_rate, reg_strength)\n",
    "\n",
    "loss, acc = eval(model_regularized)\n",
    "print('\\n\\nTest loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 4: Introduce Dropout\n",
    "\n",
    "Dropout is another regularization technique to prevent overfitting. Dropout randomly selects neurons to ignore during training. In other words, they are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. \n",
    "\n",
    "**Your task:** Remove the regularization, and this time around, add dropout for the input layer. You may want to refer to the [Keras documentation about Dropout layer](https://keras.io/layers/core/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropout_strength = 0.1\n",
    "\n",
    "#########################################################\n",
    "# Initializes neural network with dropout.\n",
    "#########################################################\n",
    "def dropout_classifier(learning_rate, dropout_strength):\n",
    "    # create model\n",
    "    model_dropout = Sequential()\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    \n",
    "    # END CODE\n",
    "    \n",
    "    # compile model\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model_dropout.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer = sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model_dropout\n",
    "\n",
    "model_dropout = dropout_classifier(learning_rate, dropout_strength)\n",
    "\n",
    "loss, acc = eval(model_dropout, verb = 2)\n",
    "print('\\n\\nTest loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
